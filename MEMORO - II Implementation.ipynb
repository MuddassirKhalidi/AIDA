{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87880c0e",
   "metadata": {},
   "source": [
    "### Test Case Situations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc335e2",
   "metadata": {},
   "source": [
    "##### S1: One conversation between two people (immediate)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S2: One conversation between two people (past)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S3: One conversation between three (or more) people (immediate)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S4: One conversation between three (or more) people (past)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S5: One conversation between two conflicting people (immediate)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S6: One conversation between two conflicting people (past)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S7: One conversation between three (or more) conflicting people (immediate)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S8: One conversation between three (or more) conflicting people (past)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S9: Ask a question which requires information from two (or more) different conversations\n",
    "- Do I know any marketing managers?\n",
    "- What are the different meetings I have had over the past week? (Specify duration)\n",
    "\n",
    "##### S10: Ask a question about information which has different forms in different conversations\n",
    "Example\n",
    "- Sarah was promoted to Head of the Marketing Department (in conversation 1)\n",
    "- Sarah was promoted to Head of the PR Department (in conversation 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e4ae7",
   "metadata": {},
   "source": [
    "### Use this cell to make installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea66d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playsound\n",
    "!pip install pyttsx3\n",
    "!pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6601349",
   "metadata": {},
   "source": [
    "### `FFmpeg` Installation\n",
    "\n",
    "#### On Windows:\n",
    "\n",
    "##### Download\n",
    "Go to the FFmpeg Official Website and download the latest build for Windows.\n",
    "\n",
    "##### Extract\n",
    "Extract the downloaded ZIP file to a directory, for example, C:\\FFmpeg.\n",
    "\n",
    "##### Environment Variable:\n",
    "- Right-click on 'This PC' or 'Computer' on your desktop or File Explorer, and select 'Properties'.\n",
    "\n",
    "- Click on 'Advanced system settings' and then 'Environment Variables'.\n",
    "\n",
    "- Under 'System Variables', find and select 'Path', then click 'Edit'.\n",
    "\n",
    "- Click 'New' and add the path to your FFmpeg bin directory, e.g., C:\\FFmpeg\\bin.\n",
    "\n",
    "- Click 'OK' to close all dialog boxes.\n",
    "\n",
    "\n",
    "#### On macOS:\n",
    "\n",
    "You can install `ffmpeg` using Homebrew:\n",
    "\n",
    "`brew install ffmpeg`\n",
    "\n",
    "#### On Linux:\n",
    "For Ubuntu and other Debian-based distributions, you can install ffmpeg from the apt repository:\n",
    "\n",
    "`sudo apt update`\n",
    "\n",
    "`sudo apt install ffmpeg`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6dd29",
   "metadata": {},
   "source": [
    "### Use this cell to import any libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb138f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI #Only for testing purposes\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from playsound import playsound\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import whisper\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69c72f",
   "metadata": {},
   "source": [
    "### Microphone Device Selection\n",
    "\n",
    "#### The `PyAudio` library requires you to choose a device with which you want to input speech. \n",
    "\n",
    "#### The function `getAudio()` has an argument `device_name`. Before running the `main` cell, change the default argument from `MacBook Pro Microphone` to the the device you want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebcabd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microphone (Jabra BIZ 1500)\n",
      "Microphone (Voicemod Virtual Au\n",
      "Microphone Array (Realtek(R) Au\n",
      "AI Noise-cancelling Input (ASUS\n",
      "Speakers (Jabra BIZ 1500)\n",
      "Speakers (Realtek(R) Audio)\n",
      "AI Noise-cancelling Output (ASU\n",
      "Line (Voicemod Virtual Audio De\n",
      "Microphone (Jabra BIZ 1500)\n",
      "Microphone (Voicemod Virtual Audio Device (WDM))\n",
      "Microphone Array (Realtek(R) Audio)\n",
      "AI Noise-cancelling Input (ASUS Utility)\n",
      "Primary Sound Driver\n",
      "Speakers (Jabra BIZ 1500)\n",
      "AI Noise-cancelling Output (ASUS Utility)\n",
      "Line (Voicemod Virtual Audio Device (WDM))\n",
      "Speakers (Realtek(R) Audio)\n",
      "AI Noise-cancelling Output (ASUS Utility)\n",
      "Line (Voicemod Virtual Audio Device (WDM))\n",
      "Speakers (Jabra BIZ 1500)\n",
      "Speakers (Realtek(R) Audio)\n",
      "Microphone (Voicemod Virtual Audio Device (WDM))\n",
      "Microphone Array (Realtek(R) Audio)\n",
      "Microphone (Jabra BIZ 1500)\n",
      "AI Noise-cancelling Input (ASUS Utility)\n",
      "Microphone (Voicemod VAD Wave)\n",
      "Line Out (Voicemod VAD Wave)\n",
      "Microphone Array (Realtek HD Audio Mic Array input)\n",
      "Microphone (Realtek HD Audio Mic input)\n",
      "Stereo Mix (Realtek HD Audio Stereo input)\n",
      "Speakers 1 (Realtek HD Audio output with HAP)\n",
      "Speakers 2 (Realtek HD Audio output with HAP)\n",
      "PC Speaker (Realtek HD Audio output with HAP)\n",
      "Headphones 1 (Realtek HD Audio 2nd output with HAP)\n",
      "Headphones 2 (Realtek HD Audio 2nd output with HAP)\n",
      "PC Speaker (Realtek HD Audio 2nd output with HAP)\n",
      "Krisp Microphone (Krisp Topology Microphone)\n",
      "Communication Speaker (Krisp Topology Microphone)\n",
      "Output (AMD HD Audio HDMI out #0)\n",
      "AI Noise-cancelling Input (Intelligo VAC (W))\n",
      "Line Out 1 (Intelligo VAC (W))\n",
      "System Virtual Line (Intelligo VAC (W))\n",
      "Line Out 2 (Intelligo VAC (W))\n",
      "Microphone (Jabra BIZ 1500)\n",
      "Speakers (Jabra BIZ 1500)\n"
     ]
    }
   ],
   "source": [
    "def list_audio_devices():\n",
    "    p = pyaudio.PyAudio()\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        print(device_info['name'])\n",
    "    p.terminate()\n",
    "\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fadae",
   "metadata": {},
   "source": [
    "### Main Code Cell\n",
    "#### Recording Audio using `pyAudio`\n",
    "#### Speech to Text using `Whisper`\n",
    "#### GPT Model: `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf184a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please start speaking. Recording...\n",
      "Audio saved to recorded_speech.wav\n",
      "Processing...\n",
      "Processed!\n",
      "Please start speaking. Recording...\n",
      "Audio saved to recorded_speech.wav\n",
      "Processing...\n",
      "Processed!\n",
      "Recognized Prompt:  What is the name I gave you? What was the name I gave you? This.\n"
     ]
    }
   ],
   "source": [
    "def get_API():\n",
    "    \"\"\"\n",
    "    Loads the OpenAI API key from the environment variables.\n",
    "\n",
    "    Returns:\n",
    "    - str: The OpenAI API key.\n",
    "    \"\"\"\n",
    "    _ = load_dotenv(find_dotenv())  # Load environment variables from .env file\n",
    "\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        raise ValueError(\"OpenAI API key is not set. Please set the 'OPENAI_API_KEY' environment variable in your .env file.\")\n",
    "    return openai_api_key\n",
    "\n",
    "def get_Model(): \n",
    "    \"\"\"\n",
    "    Determines the appropriate GPT-3.5 model based on the current date.\n",
    "\n",
    "    Returns:\n",
    "    - str: The model name to use.\n",
    "    \n",
    "    Note: We are not using this function right now \n",
    "    because the process_prompt function already decides the model\n",
    "    \"\"\"\n",
    "    current_date = datetime.datetime.now().date()\n",
    "    target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "    # Select the model based on the current date\n",
    "    if current_date > target_date:\n",
    "        llm_model = \"gpt-3.5-turbo\"\n",
    "    else:\n",
    "        llm_model = \"gpt-3.5-turbo-0301\"\n",
    "    return llm_model\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    Lists all available audio input devices.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of tuples containing device index, name, max input channels, and default sample rate.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    devices = []\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        devices.append((i, device_info['name'], device_info['maxInputChannels'], device_info['defaultSampleRate']))\n",
    "    p.terminate()\n",
    "    return devices\n",
    "\n",
    "def get_device_index_by_name(name): \n",
    "    \"\"\"\n",
    "    Finds the index of an audio device by its name.\n",
    "\n",
    "    Args:\n",
    "    - name (str): The name of the device.\n",
    "\n",
    "    Returns:\n",
    "    - int: The index of the device.\n",
    "    \n",
    "    Note: This is a helper function which will be used in getAudio()\n",
    "    \"\"\"\n",
    "    devices = list_audio_devices()\n",
    "    for index, device_name, _, _ in devices:\n",
    "        if name.lower() in device_name.lower():\n",
    "            return index\n",
    "    return None\n",
    "\n",
    "def getAudio(output_filename=\"recorded_speech.wav\", device_name=\"Microphone (Jabra BIZ 1500)\", chunk_size=1024, \n",
    "             format=pyaudio.paInt16, channels=1, rate=16000, silence_threshold=1000, silence_duration=5):\n",
    "    \"\"\"\n",
    "    Records audio until a period of silence is detected and saves it to a file.\n",
    "\n",
    "    Args:\n",
    "    - output_filename (str): Name of the output WAV file.\n",
    "    - device_name (str): Name of the input audio device.\n",
    "    - chunk_size (int): Number of frames per buffer.\n",
    "    - format: Audio format (e.g., pyaudio.paInt16).\n",
    "    - channels (int): Number of audio channels.\n",
    "    - rate (int): Sampling rate in Hz.\n",
    "    - silence_threshold (int): Amplitude threshold for silence detection.\n",
    "    - silence_duration (int): Duration of silence required to stop recording (in seconds).\n",
    "\n",
    "    Returns:\n",
    "    - str: The name of the saved audio file.\n",
    "    \n",
    "    Note: Start talking only when you the message \"Please start speaking. Recording...\" \n",
    "    If your conversation/prompt is over, but Memoro continues to record, just interrupt it\n",
    "    \"\"\"\n",
    "    device_index = get_device_index_by_name(device_name)\n",
    "    if device_index is None:\n",
    "        raise ValueError(f\"Device '{device_name}' not found.\")\n",
    "\n",
    "    # Variables to store audio frames and silence detection\n",
    "    audio_frames = []\n",
    "    silent_chunks = 0\n",
    "    max_silent_chunks = int(rate / chunk_size * silence_duration) #Formula given by ChatGPT\n",
    "\n",
    "    def is_silent(data, threshold=silence_threshold):\n",
    "        \"\"\"Returns 'True' if below the silence threshold.\"\"\"\n",
    "        max_amplitude = np.max(np.abs(data))\n",
    "        return max_amplitude < threshold\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        nonlocal silent_chunks, audio_frames\n",
    "        audio_frames.append(in_data)\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "        if is_silent(audio_data):\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        if silent_chunks > max_silent_chunks:\n",
    "            return (None, pyaudio.paComplete)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open stream\n",
    "        stream = p.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk_size,\n",
    "                        stream_callback=callback,\n",
    "                        input_device_index=device_index)\n",
    "\n",
    "        print(\"Please start speaking. Recording...\")\n",
    "        stream.start_stream()\n",
    "\n",
    "        # Keep the stream active while recording\n",
    "        while stream.is_active():\n",
    "            pass\n",
    "\n",
    "        # Stop and close the stream\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "    except KeyboardInterrupt: \n",
    "        '''\n",
    "        If you are in an environment which has a lot of noise, \n",
    "        the function may keep running even after your prompt/conversation is over. \n",
    "        Interrupt the kernel in that situation.\n",
    "        '''\n",
    "        print(\"Recording interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        p.terminate()\n",
    "\n",
    "    # Save the recorded audio to a file\n",
    "    try:\n",
    "        with wave.open(output_filename, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(audio_frames))\n",
    "        print(f\"Audio saved to {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save audio file: {e}\")\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def speech_to_text():\n",
    "    \"\"\"\n",
    "    Converts recorded audio to text using Whisper model.\n",
    "\n",
    "    Returns:\n",
    "    - str: The transcribed text.\n",
    "    \"\"\"\n",
    "    audio = getAudio()\n",
    "\n",
    "    # Suppress the FP16 warning\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(\"base\")  \n",
    "    '''\n",
    "    Choose among tiny, base, small, medium, large models\n",
    "    The higher the model, higher the accuracy. But more accuracy means \n",
    "    it will take a lot longer to transcribe the audio.\n",
    "    '''\n",
    "\n",
    "    print('Processing...')\n",
    "    # Transcribe the audio file\n",
    "    result = model.transcribe(audio)\n",
    "    print('Processed!')\n",
    "    text = result['text']\n",
    "    write_to_file(text)\n",
    "    return text\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"\n",
    "    Converts text to speech and plays the audio.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be converted to speech.\n",
    "    \"\"\"\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=text\n",
    "    )\n",
    "    file_path = 'response_voice.mp3' #Contains the audio you hear when Memoro responds\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    response.stream_to_file(file_path)\n",
    "    play_audio(file_path)\n",
    "\n",
    "def write_to_file(text):\n",
    "    \"\"\"\n",
    "    Writes the text to a file.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be written.\n",
    "\n",
    "    Returns:\n",
    "    - str: The file path.\n",
    "    \"\"\"\n",
    "    with open('STT_file.txt', 'w') as file:\n",
    "        file.write(text)\n",
    "    return os.getcwd() + '/STT_file.txt'\n",
    "\n",
    "def read_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The read text.\n",
    "    \n",
    "    Note: We are not using this function right now and may discard it after \n",
    "    integrating Memoro with PineCone\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def play_audio(file_path):\n",
    "    \"\"\"\n",
    "    Plays an audio file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the audio file.\n",
    "    \"\"\"\n",
    "    if os.name == 'nt':  # For Windows\n",
    "        os.startfile(file_path)\n",
    "    elif os.name == 'posix':  # For macOS and Linux\n",
    "        subprocess.call(['open', file_path]) if sys.platform == 'darwin' else subprocess.call(['xdg-open', file_path])\n",
    "\n",
    "def process_prompt(conversation_text):\n",
    "    \"\"\"\n",
    "    Processes the user's prompt by converting it to text, \n",
    "    querying OpenAI, and converting the response to speech.\n",
    "\n",
    "    Args:\n",
    "    - conversation_text (str): The text of the conversation.\n",
    "    \"\"\"\n",
    "    intro_path = os.path.join(os.getcwd(), 'intro', 'intro_prompt_voice.mp3')\n",
    "    play_audio(intro_path)\n",
    "    prompt = speech_to_text()\n",
    "    print(\"Recognized Prompt:\", prompt)  \n",
    "\n",
    "    if prompt:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a memory assistant, listening to my conversations.\"},\n",
    "                {\"role\": \"user\", \"content\": conversation_text},  # Provide the conversation context\n",
    "                {\"role\": \"user\", \"content\": prompt}  # User's question\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response_text = response.choices[0].message.content\n",
    "        text_to_speech(response_text)\n",
    "\n",
    "# Example usage\n",
    "api = get_API()\n",
    "text = speech_to_text()\n",
    "process_prompt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ddf67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f00d8-f6d7-4ac6-907f-4427a74990c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
