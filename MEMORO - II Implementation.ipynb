{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87880c0e",
   "metadata": {},
   "source": [
    "### Use this cell to make installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea66d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playsound\n",
    "!pip install pyttsx3\n",
    "!pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6601349",
   "metadata": {},
   "source": [
    "### `FFmpeg` Installation\n",
    "\n",
    "#### On Windows:\n",
    "\n",
    "##### Download\n",
    "Go to the FFmpeg Official Website and download the latest build for Windows.\n",
    "\n",
    "##### Extract\n",
    "Extract the downloaded ZIP file to a directory, for example, C:\\FFmpeg.\n",
    "\n",
    "##### Environment Variable:\n",
    "- Right-click on 'This PC' or 'Computer' on your desktop or File Explorer, and select 'Properties'.\n",
    "\n",
    "- Click on 'Advanced system settings' and then 'Environment Variables'.\n",
    "\n",
    "- Under 'System Variables', find and select 'Path', then click 'Edit'.\n",
    "\n",
    "- Click 'New' and add the path to your FFmpeg bin directory, e.g., C:\\FFmpeg\\bin.\n",
    "\n",
    "- Click 'OK' to close all dialog boxes.\n",
    "\n",
    "\n",
    "#### On macOS:\n",
    "\n",
    "You can install `ffmpeg` using Homebrew:\n",
    "\n",
    "`brew install ffmpeg`\n",
    "\n",
    "#### On Linux:\n",
    "For Ubuntu and other Debian-based distributions, you can install ffmpeg from the apt repository:\n",
    "\n",
    "`sudo apt update`\n",
    "\n",
    "`sudo apt install ffmpeg`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6dd29",
   "metadata": {},
   "source": [
    "### Use this cell to import any libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb138f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI #Only for testing purposes\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from playsound import playsound\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import whisper\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c69767",
   "metadata": {},
   "source": [
    "### Fetching the API Key and selecting the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f95cb711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get OpenAI API key from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OpenAI API key is not set. Please set the 'OPENAI_API_KEY' environment variable in your .env file.\")\n",
    "\n",
    "# Account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fadae",
   "metadata": {},
   "source": [
    "### Main Code Cell\n",
    "#### Recording Audio using `pyAudio`\n",
    "#### Speech to Text using `Whisper`\n",
    "#### GPT Model: `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf184a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please start speaking. Recording...\n",
      "Recording interrupted by user.\n",
      "Audio saved to recorded_speech.wav\n",
      "Processing...\n",
      "Processed!\n",
      "Please start speaking. Recording...\n",
      "Recording interrupted by user.\n",
      "Audio saved to recorded_speech.wav\n",
      "Processing...\n",
      "Processed!\n",
      "Recognized Prompt:  Where did she go to school?\n"
     ]
    }
   ],
   "source": [
    "def get_API():\n",
    "    # Load environment variables from .env file\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "\n",
    "    # Get OpenAI API key from environment variables\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        raise ValueError(\"OpenAI API key is not set. Please set the 'OPENAI_API_KEY' environment variable in your .env file.\")\n",
    "    return openai_api_key\n",
    "\n",
    "def get_Model():\n",
    "    # Get the current date\n",
    "    current_date = datetime.datetime.now().date()\n",
    "\n",
    "    # Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "    target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "    # Set the model variable based on the current date\n",
    "    if current_date > target_date:\n",
    "        llm_model = \"gpt-3.5-turbo\"\n",
    "    else:\n",
    "        llm_model = \"gpt-3.5-turbo-0301\"\n",
    "    return llm_model\n",
    "\n",
    "\n",
    "def speech_to_text():\n",
    "    audio = getAudio()\n",
    "    # Suppress the FP16 warning\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # Transcribe the audio file\n",
    "    result = model.transcribe(audio)\n",
    "    text = result['text']\n",
    "    write_to_file(text)\n",
    "    return text\n",
    "\n",
    "def list_audio_devices():\n",
    "    p = pyaudio.PyAudio()\n",
    "    devices = []\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        devices.append((i, device_info['name'], device_info['maxInputChannels'], device_info['defaultSampleRate']))\n",
    "    p.terminate()\n",
    "    return devices\n",
    "\n",
    "def get_device_index_by_name(name):\n",
    "    devices = list_audio_devices()\n",
    "    for index, device_name, _, _ in devices:\n",
    "        if name.lower() in device_name.lower():\n",
    "            return index\n",
    "    return None\n",
    "\n",
    "def speech_to_text():\n",
    "    audio = getAudio()\n",
    "    # Suppress the FP16 warning\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(\"base\")\n",
    "    print('Processing...')\n",
    "    # Transcribe the audio file\n",
    "    result = model.transcribe(audio)\n",
    "    print('Processed!')\n",
    "    text = result['text']\n",
    "    write_to_file(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def getAudio(output_filename=\"recorded_speech.wav\", device_name=\"MacBook Pro Microphone\", chunk_size=1024, \n",
    "             format=pyaudio.paInt16, channels=1, rate=16000, silence_threshold=500, silence_duration=3):\n",
    "    \"\"\"\n",
    "    Records audio until a period of silence is detected and saves it to a file.\n",
    "    \n",
    "    Args:\n",
    "    - output_filename (str): Name of the output WAV file.\n",
    "    - device_name (str): Name of the input audio device.\n",
    "    - chunk_size (int): Number of frames per buffer.\n",
    "    - format: Audio format (e.g., pyaudio.paInt16).\n",
    "    - channels (int): Number of audio channels.\n",
    "    - rate (int): Sampling rate in Hz.\n",
    "    - silence_threshold (int): Amplitude threshold for silence detection.\n",
    "    - silence_duration (int): Duration of silence required to stop recording (in seconds).\n",
    "    \n",
    "    Returns:\n",
    "    - str: The name of the saved audio file.\n",
    "    \"\"\"\n",
    "    device_index = get_device_index_by_name(device_name)\n",
    "    if device_index is None:\n",
    "        raise ValueError(f\"Device '{device_name}' not found.\")\n",
    "\n",
    "    # Variables to store audio frames and silence detection\n",
    "    audio_frames = []\n",
    "    silent_chunks = 0\n",
    "    max_silent_chunks = int(rate / chunk_size * silence_duration)\n",
    "\n",
    "    def is_silent(data, threshold=silence_threshold):\n",
    "        \"\"\"Returns 'True' if below the silence threshold.\"\"\"\n",
    "        max_amplitude = np.max(np.abs(data))\n",
    "        return max_amplitude < threshold\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        nonlocal silent_chunks, audio_frames\n",
    "        audio_frames.append(in_data)\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "        if is_silent(audio_data):\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        if silent_chunks > max_silent_chunks:\n",
    "            return (None, pyaudio.paComplete)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open stream\n",
    "        stream = p.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk_size,\n",
    "                        stream_callback=callback,\n",
    "                        input_device_index=device_index)\n",
    "\n",
    "        print(\"Please start speaking. Recording...\")\n",
    "        stream.start_stream()\n",
    "\n",
    "        # Keep the stream active while recording\n",
    "        while stream.is_active():\n",
    "            pass\n",
    "\n",
    "        # Stop and close the stream\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Recording interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        p.terminate()\n",
    "\n",
    "    # Save the recorded audio to a file\n",
    "    try:\n",
    "        with wave.open(output_filename, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(audio_frames))\n",
    "        print(f\"Audio saved to {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save audio file: {e}\")\n",
    "\n",
    "    return output_filename\n",
    "    \n",
    "def text_to_speech(text):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=text\n",
    "    )\n",
    "    file_path = 'response_voice.mp3'\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    response.stream_to_file(file_path)\n",
    "    play_audio(file_path)\n",
    "    \n",
    "def write_to_file(text):\n",
    "    '''\n",
    "    1. Write the text to a file\n",
    "    2. Returns the file path\n",
    "    '''\n",
    "    with open('STT_file.txt','w') as file:\n",
    "        file.write(text)\n",
    "    return os.getcwd() + '/STT_file.txt'\n",
    "\n",
    "def read_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def play_audio(file_path):\n",
    "    playsound(file_path)\n",
    "\n",
    "def process_prompt(conversation_text):\n",
    "    intro_path = os.path.join(os.getcwd(), 'intro', 'intro_prompt_voice.mp3')\n",
    "    play_audio(intro_path)\n",
    "    prompt = speech_to_text()\n",
    "    print(\"Recognized Prompt:\", prompt)  # Testing\n",
    "    if prompt:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a memory assistant, listening to my conversations.\"},\n",
    "                {\"role\": \"user\", \"content\": conversation_text},  # Provide the conversation context\n",
    "                {\"role\": \"user\", \"content\": prompt}  # User's question\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        text_to_speech(response_text)\n",
    "\n",
    "api = get_API()\n",
    "text = speech_to_text()\n",
    "process_prompt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ddf67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
