{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87880c0e",
   "metadata": {},
   "source": [
    "### Test Case Situations \n",
    "\n",
    "#### NOTE: Memoro II is still being tested\n",
    "\n",
    "- Add Arabic\n",
    "- Add queryless option\n",
    "- Voice detection and saving personas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc335e2",
   "metadata": {},
   "source": [
    "##### S1: One conversation between two people (immediate)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S2: One conversation between two people (past)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S3: One conversation between three (or more) people (immediate)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S4: One conversation between three (or more) people (past)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S5: One conversation between two conflicting people (immediate)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S6: One conversation between two conflicting people (past)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S7: One conversation between three (or more) conflicting people (immediate)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S8: One conversation between three (or more) conflicting people (past)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S9: Ask a question which requires information from two (or more) different conversations\n",
    "- Do I know any marketing managers?\n",
    "- What are the different meetings I have had over the past week? (Specify duration)\n",
    "\n",
    "##### S10: Ask a question about information which has different forms in different conversations\n",
    "Example\n",
    "- Sarah was promoted to Head of the Marketing Department (in conversation 1)\n",
    "- Sarah was promoted to Head of the PR Department (in conversation 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c3bcb9a",
   "metadata": {},
   "source": [
    "### Run this cell to check your `openai` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9501b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.35.14\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: /Users/muddassirkhalidi/anaconda3/lib/python3.11/site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e95e4ae7",
   "metadata": {},
   "source": [
    "### Use this cell to make installations\n",
    "\n",
    "#### You need openai version 1.35.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playsound\n",
    "!pip install -U openai\n",
    "!pip install -U openai-whisper\n",
    "!pip install pyaudio\n",
    "!pip install wave\n",
    "!pip install numpy\n",
    "!!pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6601349",
   "metadata": {},
   "source": [
    "### `FFmpeg` Installation\n",
    "\n",
    "#### On Windows:\n",
    "\n",
    "##### Download\n",
    "Go to the FFmpeg Official Website and download the latest build for Windows.\n",
    "\n",
    "##### Extract\n",
    "Extract the downloaded ZIP file to a directory, for example, C:\\FFmpeg.\n",
    "\n",
    "##### Environment Variable:\n",
    "- Right-click on 'This PC' or 'Computer' on your desktop or File Explorer, and select 'Properties'.\n",
    "\n",
    "- Click on 'Advanced system settings' and then 'Environment Variables'.\n",
    "\n",
    "- Under 'System Variables', find and select 'Path', then click 'Edit'.\n",
    "\n",
    "- Click 'New' and add the path to your FFmpeg bin directory, e.g., C:\\FFmpeg\\bin.\n",
    "\n",
    "- Click 'OK' to close all dialog boxes.\n",
    "\n",
    "\n",
    "#### On macOS:\n",
    "\n",
    "You can install `ffmpeg` using Homebrew:\n",
    "\n",
    "`brew install ffmpeg`\n",
    "\n",
    "#### On Linux:\n",
    "For Ubuntu and other Debian-based distributions, you can install ffmpeg from the apt repository:\n",
    "\n",
    "`sudo apt update`\n",
    "\n",
    "`sudo apt install ffmpeg`\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04c6dd29",
   "metadata": {},
   "source": [
    "### Use this cell to import any libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fffac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 10:51:59.620982: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from playsound import playsound\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import whisper\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.path.join(os.getcwd(), '.env'))\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc69c72f",
   "metadata": {},
   "source": [
    "### Microphone Device Selection\n",
    "\n",
    "#### The `PyAudio` library requires you to choose a device with which you want to input speech. \n",
    "\n",
    "#### Use the cell below to decide on which audio device you will use for your microphone.\n",
    "\n",
    "### RUN THIS CELL BEFORE THE MAIN CODE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebcabd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index | Device\n",
      "0     |  MacBook Pro Microphone\n",
      "1     |  MacBook Pro Speakers\n",
      "2     |  ZoomAudioDevice\n",
      "Choose a device from the list above by name: MacBook Pro Microphone\n"
     ]
    }
   ],
   "source": [
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    Lists all available audio input devices.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of tuples containing device name.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    devices = []\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        devices.append(device_info['name'])\n",
    "    p.terminate()\n",
    "    return devices\n",
    "\n",
    "def get_device_index_by_name(name): \n",
    "    \"\"\"\n",
    "    Finds the name of an audio device.\n",
    "\n",
    "    Args:\n",
    "    - name (str): The name of the device.\n",
    "\n",
    "    Returns:\n",
    "    - int: The index of the device.\n",
    "    \n",
    "    Note: This is a helper function which will be used in getAudio().\n",
    "    \"\"\"\n",
    "    devices = list_audio_devices()\n",
    "    for device_name in devices:\n",
    "        if name.lower() in device_name.lower():\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "device_list = list_audio_devices()\n",
    "devices = []\n",
    "for index, name in device_list:\n",
    "    devices.append(name)\n",
    "    print(name)\n",
    "    \n",
    "device = input('Choose a device from the list above by name: ')\n",
    "while device not in devices:\n",
    "    device = input('Choose a valid device: ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d31fadae",
   "metadata": {},
   "source": [
    "### Functions Definition Cell\n",
    "#### Recording Audio using `pyAudio`\n",
    "#### Speech to Text using `Whisper`\n",
    "#### GPT Model: `gpt-4o-mini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc653fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OPENAI_API():\n",
    "    \"\"\"\n",
    "    Loads the OpenAI API key from the environment variables.\n",
    "\n",
    "    Returns:\n",
    "    - str: The OpenAI API key.\n",
    "    \"\"\"\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai.api_key:\n",
    "        raise ValueError(\"OpenAI API key is not set. Please set the 'OPENAI_API_KEY' environment variable in your .env file.\")\n",
    "    return openai.api_key\n",
    "\n",
    "def is_positive(text, classifier):\n",
    "    # Preprocess text if necessary\n",
    "    processed_text = text  # Assuming no specific preprocessing is required\n",
    "\n",
    "    # Use Transformers pipeline for sentiment analysis\n",
    "    result = classifier(processed_text)\n",
    "\n",
    "    # Extract the sentiment label from the result\n",
    "    sentiment_label = result[0]['label']\n",
    "    if sentiment_label == 'NEGATIVE':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def getAudio(device_name=device, chunk_size=1024, \n",
    "             format=pyaudio.paInt16, channels=1, rate=16000, silence_threshold=1000, silence_duration=5):\n",
    "    \"\"\"\n",
    "    Records audio until a period of silence is detected and saves it to a file.\n",
    "\n",
    "    Args:\n",
    "    - output_filename (str): Name of the output WAV file.\n",
    "    - device_name (str): Name of the input audio device.\n",
    "    - chunk_size (int): Number of frames per buffer.\n",
    "    - format: Audio format (e.g., pyaudio.paInt16).\n",
    "    - channels (int): Number of audio channels.\n",
    "    - rate (int): Sampling rate in Hz.\n",
    "    - silence_threshold (int): Amplitude threshold for silence detection.\n",
    "    - silence_duration (int): Duration of silence required to stop recording (in seconds).\n",
    "\n",
    "    Returns:\n",
    "    - str: The name of the saved audio file.\n",
    "    \n",
    "    Note: Start talking only when you see the message \"Please start speaking. Recording...\" \n",
    "    If your conversation/prompt is over, but Memoro continues to record, just interrupt it.\n",
    "    \"\"\"\n",
    "    device_index = get_device_index_by_name(device_name)\n",
    "    if device_index is None:\n",
    "        raise ValueError(f\"Device '{device_name}' not found.\")\n",
    "\n",
    "    # Variables to store audio frames and silence detection\n",
    "    audio_frames = []\n",
    "    silent_chunks = 0\n",
    "    max_silent_chunks = int(rate / chunk_size * silence_duration)\n",
    "\n",
    "    def is_silent(data, threshold=silence_threshold):\n",
    "        \"\"\"Returns 'True' if below the silence threshold.\"\"\"\n",
    "        max_amplitude = np.max(np.abs(data))\n",
    "        return max_amplitude < threshold\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        nonlocal silent_chunks, audio_frames\n",
    "        audio_frames.append(in_data)\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "        if is_silent(audio_data):\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        if silent_chunks > max_silent_chunks:\n",
    "            return (None, pyaudio.paComplete)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open stream\n",
    "        stream = p.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk_size,\n",
    "                        stream_callback=callback,\n",
    "                        input_device_index=device_index)\n",
    "\n",
    "        print(\"Please start speaking. Recording...\")\n",
    "        stream.start_stream()\n",
    "\n",
    "        # Keep the stream active while recording\n",
    "        while stream.is_active():\n",
    "            pass\n",
    "\n",
    "        # Stop and close the stream\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "    except KeyboardInterrupt: \n",
    "        # Handle keyboard interruption for noisy environments\n",
    "        print(\"Recording interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        p.terminate()\n",
    "\n",
    "    # Save the recorded audio to a file\n",
    "    output_filename = os.path.join(os.getcwd(), 'audios','recorded_speech.wav')\n",
    "    try:\n",
    "        with wave.open(output_filename, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(audio_frames))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save audio file: {e}\")\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def play_audio(file_path):\n",
    "    \"\"\"\n",
    "    Plays an audio file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the audio file.\n",
    "    \"\"\"\n",
    "    playsound(file_path)\n",
    "    \n",
    "def speech_to_text():\n",
    "    \"\"\"\n",
    "    Converts recorded audio to text using Whisper model.\n",
    "\n",
    "    Returns:\n",
    "    - str: The transcribed text.\n",
    "    \"\"\"\n",
    "    audio = getAudio()\n",
    "\n",
    "    # Suppress the FP16 warning\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(\"base\")  \n",
    "    '''\n",
    "    Choose among tiny, base, small, medium, large models\n",
    "    The higher the model, higher the accuracy. But more accuracy means \n",
    "    it will take a lot longer to transcribe the audio.\n",
    "    '''\n",
    "\n",
    "    print('Processing speech...')\n",
    "    # Transcribe the audio file\n",
    "    result = model.transcribe(audio)\n",
    "    print('Transcribed!')\n",
    "    text = result['text']\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"\n",
    "    Converts text to speech and plays the audio.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be converted to speech.\n",
    "    \"\"\"\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=text\n",
    "    )\n",
    "    response_path = os.path.join(os.getcwd(), 'audios', 'response_voice.mp3')  # Contains the audio you hear when Memoro responds\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    response.stream_to_file(response_path)\n",
    "    play_audio(response_path)\n",
    "\n",
    "def write_to_file(text):\n",
    "    \"\"\"\n",
    "    Writes the text to a file.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be written.\n",
    "\n",
    "    Returns:\n",
    "    - str: The file path.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(os.getcwd(), 'buffer', 'short_term_buffer.txt')\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(text)\n",
    "        \n",
    "    return \n",
    "\n",
    "def read_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The read text.\n",
    "    \n",
    "    Note: We are not using this function right now and may discard it after \n",
    "    integrating Memoro II with PineCone.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def get_context():\n",
    "    text = speech_to_text()\n",
    "    context = f'\\n\\nTimestamp: {str(datetime.now())}\\nConversation:\\n{text}'\n",
    "    write_to_file(context)\n",
    "\n",
    "def get_prompt():\n",
    "    query = speech_to_text()\n",
    "    prompt = f'\\n\\nTimestamp: {str(datetime.now())}\\nQuestion: {query}'\n",
    "    write_to_file(prompt)\n",
    "    short_buffer = os.path.join(os.getcwd(), 'buffer', 'short_term_buffer.txt')\n",
    "    long_buffer = os.path.join(os.getcwd(), 'buffer', 'long_term_buffer.txt')\n",
    "    for file in [short_buffer, long_buffer]:\n",
    "        context = read_from_file(file)\n",
    "        response = openai.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Your name is Memoro and you are a memory assistant listening to my conversations. You are given context with timstamps for the different conversations I have had. After you answer the question, ask for a follow up question.\"},\n",
    "                {\"role\": \"user\", \"content\": context},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "\n",
    "            ]\n",
    "        )\n",
    "        text = response.choices[0].message.content\n",
    "        if is_positive(text, classifier):\n",
    "            break\n",
    "        text_to_speech('Searching long term buffer...')\n",
    "            \n",
    "#     response = text + '\\nIs there anything else I can help you with?'\n",
    "    text_to_speech(text)\n",
    "    print(text)\n",
    "\n",
    "def buffer_exceeded():\n",
    "    short_buffer = os.path.join(os.getcwd(), 'buffer', 'short_term_buffer.txt')\n",
    "    text = read_from_file(short_buffer)\n",
    "    # Define the tokenizer for GPT-3.5-turbo using cl100k_base encoding\n",
    "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "    # Encode the context and count the tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    num_tokens = len(tokens)\n",
    "    if num_tokens > 2000:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def move_to_long_term_buffer():\n",
    "    # Read the entire content of the short term buffer file\n",
    "    short_term_buffer = os.path.join(os.getcwd(), 'buffer', 'short_term_buffer.txt')\n",
    "    long_term_buffer = os.path.join(os.getcwd(), 'buffer', 'long_term_buffer.txt')\n",
    "    with open(short_term_buffer, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    # Split the tokens into two parts\n",
    "    first_n_tokens = tokens[:2000]\n",
    "    remaining_tokens = tokens[2000:]\n",
    "\n",
    "    # Decode the first 2000 tokens and remaining tokens back to text\n",
    "    first_n_text = tokenizer.decode(first_n_tokens)\n",
    "    remaining_text = tokenizer.decode(remaining_tokens)\n",
    "\n",
    "    # Write the first n tokens to the new file\n",
    "    with open(long_term_buffer, 'a', encoding='utf-8') as file:\n",
    "        file.write(first_n_text)\n",
    "\n",
    "    # Update the original file with the remaining tokens\n",
    "    with open(short_term_buffer, 'w', encoding='utf-8') as file:\n",
    "        file.write(remaining_text)\n",
    "        \n",
    "def intro():\n",
    "    file_path = os.path.join(os.getcwd(), 'audios','intro_prompt_voice.mp3')\n",
    "    play_audio(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e1ffcce",
   "metadata": {},
   "source": [
    "# Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689a5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 to record and 2 to retrieve memories: 1\n",
      "Please start speaking. Recording...\n",
      "Processing speech...\n",
      "Transcribed!\n",
      " Starts me. Let's talk. Whatever talk. I see how long it records. Does it have an SD thing? Okay, I'll stop this. I have a memorial.\n",
      "--------------------------------------------------\n",
      "Interrupt the kernel to end the program\n",
      "Thank you!\n"
     ]
    }
   ],
   "source": [
    "intro()\n",
    "\n",
    "while True:\n",
    "    if buffer_exceeded():\n",
    "        move_to_long_term_buffer()\n",
    "    \n",
    "    try:\n",
    "        choice = input('Enter 1 to record and 2 to retrieve memories: ')\n",
    "        while choice not in ['1','2']:\n",
    "            choice = input('Enter a valid choice [1,2]: ')\n",
    "        if choice == '1':\n",
    "            get_context()\n",
    "        else:\n",
    "            get_prompt()\n",
    "        print('-'*50)\n",
    "        print('Interrupt the kernel to end the program')\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Thank you!\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66cd7d8a",
   "metadata": {},
   "source": [
    "### Testing Cases\n",
    "\n",
    "#### Use the functions below to get audio files from the text files. \n",
    "- Get the content for the text files from ChatGPT\n",
    "- After getting the content, create text files in the MEMORO-II/memoro-ii/test_files/text\n",
    "- Name the text files like this: S{test case number}.txt\n",
    "- Once the text files are created, run the cell below.\n",
    "- Wait for some time for the cell to complete running, this will take some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea2173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The read text.\n",
    "    \n",
    "    Note: We are not using this function right now and may discard it after \n",
    "    integrating Memoro II with PineCone.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def text_to_speech(text, index):\n",
    "    \"\"\"\n",
    "    Converts text to speech and plays the audio.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be converted to speech.\n",
    "    \"\"\"\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=text\n",
    "    )\n",
    "    response_path = os.path.join(os.getcwd(),'test_files', 'text', f'S{index}.mp3')  # Contains the audio you hear when Memoro responds\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    response.stream_to_file(response_path)\n",
    "#     play_audio(response_path)\n",
    "\n",
    "#Change the range to (6,11) for test cases 6-10\n",
    "for x in range(1,6):\n",
    "    text = read_from_file(os.path.join(os.getcwd(), 'test_files', 'audio' f'S{x}.txt'))\n",
    "    print(text)\n",
    "    text_to_speech(text, x)\n",
    "    print('-'*50)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
