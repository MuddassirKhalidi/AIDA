{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6926a097-e22d-4477-a35e-ea48b45d3a45",
   "metadata": {},
   "source": [
    "### use this cell for `installations` of the relevant versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb601f7-73ac-48ea-b683-a13a0e5c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    openai==0.27.7 \\\n",
    "    pandas==2.0.3\\\n",
    "    datasets==2.12.0 \\\n",
    "    pinecone-client==3.2.2 \\\n",
    "    pinecone-datasets==0.7.0 \\\n",
    "    pinecone-notebooks==0.1.1\\\n",
    "    tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10922159-d3f6-441a-b472-2ccd8fe341a5",
   "metadata": {},
   "source": [
    "### OpenAI GPT-3.5-turbo-instruct Text Completion Script and setting up openAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86acf1-f722-4958-a973-22dfe5f4ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# get API key from top-right dropdown on OpenAI website\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"sk-proj-cns0EYsVd8MwGmNqImmMT3BlbkFJ2nrjtYwMUPXe2lZXAngA\"\n",
    "\n",
    "openai.Engine.list()\n",
    "\n",
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='gpt-3.5-turbo-instruct',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2aab03-670d-434e-aab3-db5ad1c9608d",
   "metadata": {},
   "source": [
    "### Loading, splitting, and reading data from out text file using nltk and then using 'text-embedding-ada-002' for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c2ac3-520c-42d0-b5e2-08a267ecafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "import openai\n",
    "from time import sleep\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'sk-proj-cns0EYsVd8MwGmNqImmMT3BlbkFJ2nrjtYwMUPXe2lZXAngA'\n",
    "\n",
    "# Function to read text from a file and split it into sentences\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Function to create embeddings for a batch of sentences\n",
    "def create_embeddings(sentences, embed_model='text-embedding-ada-002', batch_size=10):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        try:\n",
    "            res = openai.Embedding.create(input=batch, engine=embed_model)\n",
    "            embeddings.extend(res['data'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embeddings for batch {i//batch_size + 1}: {e}\")\n",
    "            # Handle retries or wait times here if needed\n",
    "            sleep(5)  # Example: retry after 5 seconds\n",
    "            continue\n",
    "    return embeddings\n",
    "\n",
    "# Path to your text file\n",
    "file_path = 'text.txt'\n",
    "\n",
    "# Load text data and split into sentences\n",
    "sentences = load_text_file(file_path)\n",
    "\n",
    "# Example batch processing and embedding creation\n",
    "batch_size = 10\n",
    "embeddings = create_embeddings(sentences, batch_size=batch_size)\n",
    "\n",
    "# Example: Print embeddings for the first few batches\n",
    "for i, embedding in enumerate(embeddings[:10]):\n",
    "    print(f\"Embedding {i+1}: {embedding}\")\n",
    "\n",
    "# Placeholder for new merged data (your original processing logic)\n",
    "new_data = []\n",
    "\n",
    "window = 20  # number of sentences to combine\n",
    "stride = 4   # number of sentences to 'stride' over\n",
    "\n",
    "for i in tqdm(range(0, len(sentences), stride)):\n",
    "    i_end = min(len(sentences)-1, i+window)\n",
    "    text = ' '.join(sentences[i:i_end])\n",
    "    new_data.append({\n",
    "        'start': i,\n",
    "        'end': i_end,\n",
    "        'text': text,\n",
    "        'id': i,  # Placeholder ID\n",
    "        'url': 'N/A',\n",
    "        'published': 'N/A',\n",
    "        'channel_id': 'N/A'\n",
    "    })\n",
    "\n",
    "# Example: Print the first few entries of the new dataset\n",
    "#for entry in new_data[:5]:\n",
    "    #print(entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e0ac0-41ae-4a0f-9d6b-9232dec8d559",
   "metadata": {},
   "source": [
    "### Setting up Pinecone API and environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3ccc5-d59c-4bb1-b2bb-2d01f10661b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "use_serverless = True\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.environ.get('PINECONE_API_KEY') or '1fdff541-89da-47de-8a43-71d45f8afd2d'\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "\n",
    "index_name = \"memoro\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d3a8f-4470-449e-92e8-35cbc363f91a",
   "metadata": {},
   "source": [
    "### Pinecone Index Initialization and Connection Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375b3aa-c705-45f6-8a8e-659f92edeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=len(res['data'][0]['embedding']),\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d172a3-bb68-4447-9d46-11be63caff54",
   "metadata": {},
   "source": [
    "### Batch Processing and Upserting Embeddings into Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643366a-a1dc-47bc-9dfd-58525ffac0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "import openai\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = 'sk-proj-cns0EYsVd8MwGmNqImmMT3BlbkFJ2nrjtYwMUPXe2lZXAngA'\n",
    "\n",
    "# Parameters\n",
    "batch_size = 100  # Number of embeddings to create and insert at once\n",
    "embed_model = 'text-embedding-ada-002'  # OpenAI embedding model\n",
    "\n",
    "# Iterate over new_data in batches and create embeddings\n",
    "for i in tqdm(range(0, len(new_data), batch_size)):\n",
    "    # Determine end of current batch\n",
    "    i_end = min(len(new_data), i + batch_size)\n",
    "    meta_batch = new_data[i:i_end]\n",
    "    \n",
    "    # Extract IDs and texts from meta_batch\n",
    "    ids_batch = [str(x['id']) for x in meta_batch]  # Ensure id is converted to string\n",
    "    texts = [x['text'] for x in meta_batch]\n",
    "    \n",
    "    # Create embeddings (with retry logic)\n",
    "    done = False\n",
    "    while not done:\n",
    "        try:\n",
    "            res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "            done = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embeddings for batch {i // batch_size + 1}: {e}\")\n",
    "            sleep(5)  # Wait before retrying\n",
    "            continue\n",
    "    \n",
    "    # Extract embeddings and prepare metadata for upsert\n",
    "    embeds = [record['embedding'] for record in res['data']]\n",
    "    meta_batch = [{\n",
    "        'start': x['start'],\n",
    "        'end': x['end'],\n",
    "        'title': x.get('title', 'N/A'),  # Replace with actual metadata fields if available\n",
    "        'text': x['text'],\n",
    "        'url': x.get('url', 'N/A'),\n",
    "        'published': x.get('published', 'N/A'),\n",
    "        'channel_id': x.get('channel_id', 'N/A')\n",
    "    } for x in meta_batch]\n",
    "    \n",
    "    # Prepare data for upsert into Pinecone\n",
    "    to_upsert = [(ids_batch[j], embeds[j], meta_batch[j]) for j in range(len(meta_batch))]\n",
    "    \n",
    "    # Upsert vectors into Pinecone index\n",
    "    try:\n",
    "        index.upsert(vectors=to_upsert)\n",
    "        print(f\"Successfully upserted batch {i // batch_size + 1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting vectors for batch {i // batch_size + 1}: {e}\")\n",
    "        # Handle retry or error recovery logic here if needed\n",
    "query = (\n",
    "    \"What natural disasters are caused by climate change?\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552564c-bd40-4b90-9b58-665beb8e1d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = openai.Embedding.create(\n",
    "    input=[query],\n",
    "    engine=embed_model\n",
    ")\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res['data'][0]['embedding']\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(vector=xq, top_k=2, include_metadata=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f952039-aed5-44c2-9226-03e3bfcdc3ce",
   "metadata": {},
   "source": [
    "### Retrieving and Constructing a Prompt with Context from Pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8f2bc-acc4-4d15-b426-db176963f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 3750\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=embed_model\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ff9d0-e979-46ef-b8c0-00f96bcbf69c",
   "metadata": {},
   "source": [
    "### Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1980b-a4b9-475c-acfe-d79b75d98398",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"What natural disasters are caused by climate change?\" \n",
    ")\n",
    "query_with_contexts = retrieve(query)\n",
    "query_with_contexts   # provides context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51abc667-9365-46ac-8bfd-073b77efa859",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete(query_with_contexts) # provides answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb659d7-24c6-4433-adc1-27bcc45dc3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
