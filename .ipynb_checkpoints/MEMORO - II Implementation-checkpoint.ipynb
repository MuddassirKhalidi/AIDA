{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83c06548",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2093574545.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install -U langchain-openai\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install openai\n",
    "# !pip install SpeechRecognition pyaudio nltk scikit-learn semantic-text-splitter gap-statistic tokenizers transformers \n",
    "# !pip install --upgrade langchain\n",
    "# !pip install langchain_community\n",
    "# !pip install sentence-transformers\n",
    "# !pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7b98a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/pocketsphinx/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/pocketsphinx/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/pocketsphinx/\u001b[0m\u001b[33m\n",
      "\u001b[0m^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pocketsphinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04fecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get OpenAI API key from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OpenAI API key is not set. Please set the 'OPENAI_API_KEY' environment variable in your .env file.\")\n",
    "\n",
    "# Account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23c1b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to recognize speech from the microphone\n",
    "def recognize_speech_from_mic():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Adjusting for ambient noise... Please wait.\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        print(\"Listening for speech...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            print(\"Recognizing speech...\")\n",
    "            # Use the Google Web Speech API for recognition\n",
    "            text = recognizer.recognize_whisper\n",
    "            print(f\"Recognized Text: {text}\")\n",
    "            \n",
    "            # Save the recognized text to a file\n",
    "            with open(\"recognized_speech.txt\", \"w\") as file:\n",
    "                file.write(text + \"\\n\")\n",
    "                \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Web Speech API; {e}\")\n",
    "\n",
    "# recognize_speech_from_mic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "828727d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the prompt you want to ask the text file?\n",
      "Adjusting for ambient noise... Please wait.\n",
      "Listening for question...\n",
      "Recognizing question...\n",
      "Could not request results from Google Web Speech API; missing PocketSphinx module: ensure that PocketSphinx is set up correctly.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'prompt' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not request results from Google Web Speech API; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n\u001b[0;32m---> 26\u001b[0m question \u001b[38;5;241m=\u001b[39m prompt_from_mic()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m question:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Initialize the ChatOpenAI model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 23\u001b[0m, in \u001b[0;36mprompt_from_mic\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not request results from Google Web Speech API; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'prompt' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "file_path = 'test_file.txt' \n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "print(\"what is the prompt you want to ask the text file?\")\n",
    "\n",
    "def prompt_from_mic():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Adjusting for ambient noise... Please wait.\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        print(\"Listening for question...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            print(\"Recognizing question...\")\n",
    "            # Use the Google Web Speech API for recognition\n",
    "            prompt = recognizer.recognize_sphinx(audio)\n",
    "            print(f\"Recognized Text: {prompt}\")\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Web Speech API; {e}\")\n",
    "    return prompt\n",
    "\n",
    "    \n",
    "question = prompt_from_mic()\n",
    "if question:\n",
    "    # Initialize the ChatOpenAI model\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Question: {question}\\n\\nText Data: {text_data}\"\n",
    "    )\n",
    "\n",
    "    # Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # Run the chain with both question and text_data\n",
    "    result = chain.run({\"question\": question, \"text_data\": text_data})\n",
    "\n",
    "    # Print the result\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"No question recognized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e837de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting for ambient noise... Please wait.\n",
      "Listening for speech...\n",
      "Recognizing speech...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soundfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not request results from Google Web Speech API; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m speech_to_text()\n",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m, in \u001b[0;36mspeech_to_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecognizing speech...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;66;03m# Use the OpenAI Whisper Speech API for recognition\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m             text \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_whisper(audio)\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecognized Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;66;03m# Save the recognized text to a file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#             with open(\"recognized_speech.txt\", \"w\") as file:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#                 file.write(text + \"\\n\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speech_recognition/__init__.py:1406\u001b[0m, in \u001b[0;36mRecognizer.recognize_whisper\u001b[0;34m(self, audio_data, model, show_dict, load_options, language, translate, **transcribe_options)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio_data, AudioData), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be audio data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoundfile\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msf\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'soundfile'"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import soundfile as sf\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def speech_to_text():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Adjusting for ambient noise... Please wait.\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        print(\"Listening for speech...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            print(\"Recognizing speech...\")\n",
    "            # Use the OpenAI Whisper Speech API for recognition\n",
    "            text = recognizer.recognize_whisper(audio)\n",
    "            print(f\"Recognized Text: {text}\")\n",
    "            \n",
    "            # Save the recognized text to a file\n",
    "#             with open(\"recognized_speech.txt\", \"w\") as file:\n",
    "#                 file.write(text + \"\\n\")\n",
    "                \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Web Speech API; {e}\")\n",
    "speech_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3fc99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
