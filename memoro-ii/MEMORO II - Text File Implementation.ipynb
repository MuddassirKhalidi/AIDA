{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87880c0e",
   "metadata": {},
   "source": [
    "### Test Case Situations \n",
    "\n",
    "#### NOTE: Memoro II is still being tested\n",
    "\n",
    "- Add Arabic\n",
    "- Add queryless option\n",
    "- Voice detection and saving personas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc335e2",
   "metadata": {},
   "source": [
    "##### S1: One conversation between two people (immediate)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S2: One conversation between two people (past)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S3: One conversation between three (or more) people (immediate)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S4: One conversation between three (or more) people (past)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S5: One conversation between two conflicting people (immediate)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S6: One conversation between two conflicting people (past)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S7: One conversation between three (or more) conflicting people (immediate)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S8: One conversation between three (or more) conflicting people (past)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S9: Ask a question which requires information from two (or more) different conversations\n",
    "- Do I know any marketing managers?\n",
    "- What are the different meetings I have had over the past week? (Specify duration)\n",
    "\n",
    "##### S10: Ask a question about information which has different forms in different conversations\n",
    "Example\n",
    "- Sarah was promoted to Head of the Marketing Department (in conversation 1)\n",
    "- Sarah was promoted to Head of the PR Department (in conversation 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bcb9a",
   "metadata": {},
   "source": [
    "### Run this cell to check your `openai` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9501b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.35.14\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: /Users/muddassirkhalidi/anaconda3/lib/python3.11/site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e4ae7",
   "metadata": {},
   "source": [
    "### Use this cell to make installations\n",
    "\n",
    "#### You need openai version 1.35.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playsound\n",
    "!pip install -U openai\n",
    "!pip install -U openai-whisper\n",
    "!pip install pyaudio\n",
    "!pip install wave\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install pinecone\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6601349",
   "metadata": {},
   "source": [
    "### `FFmpeg` Installation\n",
    "\n",
    "#### On Windows:\n",
    "\n",
    "##### Download\n",
    "Go to the FFmpeg Official Website and download the latest build for Windows.\n",
    "\n",
    "##### Extract\n",
    "Extract the downloaded ZIP file to a directory, for example, C:\\FFmpeg.\n",
    "\n",
    "##### Environment Variable:\n",
    "- Right-click on 'This PC' or 'Computer' on your desktop or File Explorer, and select 'Properties'.\n",
    "\n",
    "- Click on 'Advanced system settings' and then 'Environment Variables'.\n",
    "\n",
    "- Under 'System Variables', find and select 'Path', then click 'Edit'.\n",
    "\n",
    "- Click 'New' and add the path to your FFmpeg bin directory, e.g., C:\\FFmpeg\\bin.\n",
    "\n",
    "- Click 'OK' to close all dialog boxes.\n",
    "\n",
    "\n",
    "#### On macOS:\n",
    "\n",
    "You can install `ffmpeg` using Homebrew:\n",
    "\n",
    "`brew install ffmpeg`\n",
    "\n",
    "#### On Linux:\n",
    "For Ubuntu and other Debian-based distributions, you can install ffmpeg from the apt repository:\n",
    "\n",
    "`sudo apt update`\n",
    "\n",
    "`sudo apt install ffmpeg`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6dd29",
   "metadata": {},
   "source": [
    "### Use this cell to import any libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fffac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/muddassirkhalidi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from playsound import playsound\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import whisper\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pinecone\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "nltk.download('punkt')\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.path.join(os.getcwd(), '.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69c72f",
   "metadata": {},
   "source": [
    "### Microphone Device Selection\n",
    "\n",
    "#### The `PyAudio` library requires you to choose a device with which you want to input speech. \n",
    "\n",
    "#### The function `getAudio()` has an argument `device_name`. Before running the `main` cell, change the \n",
    "\n",
    "#### default argument from `MacBook Pro Microphone` to the the device you want to use. \n",
    "\n",
    "\n",
    "### RUN THIS CELL BEFORE THE MAIN CODE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebcabd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone Microphone\n",
      "MacBook Pro Microphone\n",
      "MacBook Pro Speakers\n",
      "ZoomAudioDevice\n"
     ]
    }
   ],
   "source": [
    "def list_audio_devices():\n",
    "    p = pyaudio.PyAudio()\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        print(device_info['name'])\n",
    "    p.terminate()\n",
    "\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fadae",
   "metadata": {},
   "source": [
    "### Main Code Cell\n",
    "#### Recording Audio using `pyAudio`\n",
    "#### Speech to Text using `Whisper`\n",
    "#### GPT Model: `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9996133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OPENAI_API():\n",
    "    \"\"\"\n",
    "    Loads the OpenAI API key from the environment variables.\n",
    "\n",
    "    Returns:\n",
    "    - str: The OpenAI API key.\n",
    "    \"\"\"\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai.api_key:\n",
    "        raise ValueError(\"OpenAI API key is not set. Please set the 'OPENAI_API_KEY' environment variable in your .env file.\")\n",
    "    return openai.api_key\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    Lists all available audio input devices.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of tuples containing device index, name, max input channels, and default sample rate.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    devices = []\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        devices.append((i, device_info['name'], device_info['maxInputChannels'], device_info['defaultSampleRate']))\n",
    "    p.terminate()\n",
    "    return devices\n",
    "\n",
    "def get_device_index_by_name(name): \n",
    "    \"\"\"\n",
    "    Finds the index of an audio device by its name.\n",
    "\n",
    "    Args:\n",
    "    - name (str): The name of the device.\n",
    "\n",
    "    Returns:\n",
    "    - int: The index of the device.\n",
    "    \n",
    "    Note: This is a helper function which will be used in getAudio().\n",
    "    \"\"\"\n",
    "    devices = list_audio_devices()\n",
    "    for index, device_name, _, _ in devices:\n",
    "        if name.lower() in device_name.lower():\n",
    "            return index\n",
    "    return None\n",
    "\n",
    "def getAudio(output_filename=\"recorded_speech.wav\", device_name=\"MacBook Pro Microphone\", chunk_size=1024, \n",
    "             format=pyaudio.paInt16, channels=1, rate=16000, silence_threshold=1000, silence_duration=5):\n",
    "    \"\"\"\n",
    "    Records audio until a period of silence is detected and saves it to a file.\n",
    "\n",
    "    Args:\n",
    "    - output_filename (str): Name of the output WAV file.\n",
    "    - device_name (str): Name of the input audio device.\n",
    "    - chunk_size (int): Number of frames per buffer.\n",
    "    - format: Audio format (e.g., pyaudio.paInt16).\n",
    "    - channels (int): Number of audio channels.\n",
    "    - rate (int): Sampling rate in Hz.\n",
    "    - silence_threshold (int): Amplitude threshold for silence detection.\n",
    "    - silence_duration (int): Duration of silence required to stop recording (in seconds).\n",
    "\n",
    "    Returns:\n",
    "    - str: The name of the saved audio file.\n",
    "    \n",
    "    Note: Start talking only when you see the message \"Please start speaking. Recording...\" \n",
    "    If your conversation/prompt is over, but Memoro continues to record, just interrupt it.\n",
    "    \"\"\"\n",
    "    device_index = get_device_index_by_name(device_name)\n",
    "    if device_index is None:\n",
    "        raise ValueError(f\"Device '{device_name}' not found.\")\n",
    "\n",
    "    # Variables to store audio frames and silence detection\n",
    "    audio_frames = []\n",
    "    silent_chunks = 0\n",
    "    max_silent_chunks = int(rate / chunk_size * silence_duration)\n",
    "\n",
    "    def is_silent(data, threshold=silence_threshold):\n",
    "        \"\"\"Returns 'True' if below the silence threshold.\"\"\"\n",
    "        max_amplitude = np.max(np.abs(data))\n",
    "        return max_amplitude < threshold\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        nonlocal silent_chunks, audio_frames\n",
    "        audio_frames.append(in_data)\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "        if is_silent(audio_data):\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        if silent_chunks > max_silent_chunks:\n",
    "            return (None, pyaudio.paComplete)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open stream\n",
    "        stream = p.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk_size,\n",
    "                        stream_callback=callback,\n",
    "                        input_device_index=device_index)\n",
    "\n",
    "        print(\"Please start speaking. Recording...\")\n",
    "        stream.start_stream()\n",
    "\n",
    "        # Keep the stream active while recording\n",
    "        while stream.is_active():\n",
    "            pass\n",
    "\n",
    "        # Stop and close the stream\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "    except KeyboardInterrupt: \n",
    "        # Handle keyboard interruption for noisy environments\n",
    "        print(\"Recording interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        p.terminate()\n",
    "\n",
    "    # Save the recorded audio to a file\n",
    "    try:\n",
    "        with wave.open(output_filename, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(audio_frames))\n",
    "        print(f\"Audio saved to {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save audio file: {e}\")\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def play_audio(file_path):\n",
    "    \"\"\"\n",
    "    Plays an audio file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the audio file.\n",
    "    \"\"\"\n",
    "    playsound(file_path)\n",
    "    \n",
    "def speech_to_text():\n",
    "    \"\"\"\n",
    "    Converts recorded audio to text using Whisper model.\n",
    "\n",
    "    Returns:\n",
    "    - str: The transcribed text.\n",
    "    \"\"\"\n",
    "    audio = getAudio()\n",
    "\n",
    "    # Suppress the FP16 warning\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(\"base\")  \n",
    "    '''\n",
    "    Choose among tiny, base, small, medium, large models\n",
    "    The higher the model, higher the accuracy. But more accuracy means \n",
    "    it will take a lot longer to transcribe the audio.\n",
    "    '''\n",
    "\n",
    "    print('Processing speech...')\n",
    "    # Transcribe the audio file\n",
    "    result = model.transcribe(audio)\n",
    "    print('Transcribed!')\n",
    "    text = result['text']\n",
    "    print(text)\n",
    "    write_to_file(text)\n",
    "    return text\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"\n",
    "    Converts text to speech and plays the audio.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be converted to speech.\n",
    "    \"\"\"\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=text\n",
    "    )\n",
    "    response_path = os.path.join(os.getcwd(), 'response_voice.mp3')  # Contains the audio you hear when Memoro responds\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    response.stream_to_file(response_path)\n",
    "    play_audio(response_path)\n",
    "\n",
    "def write_to_file(text):\n",
    "    \"\"\"\n",
    "    Writes the text to a file.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be written.\n",
    "\n",
    "    Returns:\n",
    "    - str: The file path.\n",
    "    \"\"\"\n",
    "    with open('context.txt', 'a') as file:\n",
    "        file.write(text)\n",
    "        \n",
    "    return os.path.join(os.getcwd(), 'context.txt')\n",
    "\n",
    "def read_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The read text.\n",
    "    \n",
    "    Note: We are not using this function right now and may discard it after \n",
    "    integrating Memoro II with PineCone.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def get_context():\n",
    "    text = speech_to_text()\n",
    "    context = f'\\n\\nTimestamp: {str(datetime.now())}\\nConversation:\\n{text}'\n",
    "    write_to_file(context)\n",
    "\n",
    "def get_prompt():\n",
    "    query = speech_to_text()\n",
    "    prompt = f'\\n\\nTimestamp: {str(datetime.now())}\\nQuestion: {query}'\n",
    "    write_to_file(prompt)\n",
    "    context = read_from_file('context.txt')\n",
    "    response = openai.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a memory assistant listening to my conversations. Answer the question based on the context which is organized by the date and time of the conversation.\"},\n",
    "            {\"role\": \"user\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "    text = response.choices[0].message.content\n",
    "    text_to_speech(text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f12a4d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please start speaking. Recording...\n",
      "Recording interrupted by user.\n",
      "Audio saved to recorded_speech.wav\n",
      "Processing speech...\n",
      "Transcribed!\n",
      " What position is Sarah being promoted to?\n",
      "Sarah is being promoted to the head of the sales department.\n"
     ]
    }
   ],
   "source": [
    "get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208e390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
