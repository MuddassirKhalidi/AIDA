{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87880c0e",
   "metadata": {},
   "source": [
    "### Test Case Situations \n",
    "\n",
    "#### NOTE: Memoro II is still being tested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc335e2",
   "metadata": {},
   "source": [
    "##### S1: One conversation between two people (immediate)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S2: One conversation between two people (past)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S3: One conversation between three (or more) people (immediate)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S4: One conversation between three (or more) people (past)\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about things which have been mentioned at different instances\n",
    "\n",
    "##### S5: One conversation between two conflicting people (immediate)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S6: One conversation between two conflicting people (past)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S7: One conversation between three (or more) conflicting people (immediate)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S8: One conversation between three (or more) conflicting people (past)\n",
    "The two people have a conflicting \"opinion\" about a subject\n",
    "- Prompt to ask direct questions about the conversation\n",
    "- Prompt to ask questions about the opinion of either of the speakers\n",
    "\n",
    "##### S9: Ask a question which requires information from two (or more) different conversations\n",
    "- Do I know any marketing managers?\n",
    "- What are the different meetings I have had over the past week? (Specify duration)\n",
    "\n",
    "##### S10: Ask a question about information which has different forms in different conversations\n",
    "Example\n",
    "- Sarah was promoted to Head of the Marketing Department (in conversation 1)\n",
    "- Sarah was promoted to Head of the PR Department (in conversation 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bcb9a",
   "metadata": {},
   "source": [
    "### Run this cell to check your `openai` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9501b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.35.13\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: /Users/muddassirkhalidi/anaconda3/lib/python3.11/site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e4ae7",
   "metadata": {},
   "source": [
    "### Use this cell to make installations\n",
    "\n",
    "#### You need openai version 1.35.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playsound\n",
    "!pip install -U openai\n",
    "!pip install -U openai-whisper\n",
    "!pip install pyaudio\n",
    "!pip install wave\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install pinecone\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6601349",
   "metadata": {},
   "source": [
    "### `FFmpeg` Installation\n",
    "\n",
    "#### On Windows:\n",
    "\n",
    "##### Download\n",
    "Go to the FFmpeg Official Website and download the latest build for Windows.\n",
    "\n",
    "##### Extract\n",
    "Extract the downloaded ZIP file to a directory, for example, C:\\FFmpeg.\n",
    "\n",
    "##### Environment Variable:\n",
    "- Right-click on 'This PC' or 'Computer' on your desktop or File Explorer, and select 'Properties'.\n",
    "\n",
    "- Click on 'Advanced system settings' and then 'Environment Variables'.\n",
    "\n",
    "- Under 'System Variables', find and select 'Path', then click 'Edit'.\n",
    "\n",
    "- Click 'New' and add the path to your FFmpeg bin directory, e.g., C:\\FFmpeg\\bin.\n",
    "\n",
    "- Click 'OK' to close all dialog boxes.\n",
    "\n",
    "\n",
    "#### On macOS:\n",
    "\n",
    "You can install `ffmpeg` using Homebrew:\n",
    "\n",
    "`brew install ffmpeg`\n",
    "\n",
    "#### On Linux:\n",
    "For Ubuntu and other Debian-based distributions, you can install ffmpeg from the apt repository:\n",
    "\n",
    "`sudo apt update`\n",
    "\n",
    "`sudo apt install ffmpeg`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6dd29",
   "metadata": {},
   "source": [
    "### Use this cell to import any libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fffac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from playsound import playsound\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import whisper\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pinecone\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "nltk.download('punkt')\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.path.join(os.getcwd(), '.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69c72f",
   "metadata": {},
   "source": [
    "### Microphone Device Selection\n",
    "\n",
    "#### The `PyAudio` library requires you to choose a device with which you want to input speech. \n",
    "\n",
    "#### The function `getAudio()` has an argument `device_name`. Before running the `main` cell, change the \n",
    "\n",
    "#### default argument from `MacBook Pro Microphone` to the the device you want to use. \n",
    "\n",
    "\n",
    "### RUN THIS CELL BEFORE THE MAIN CODE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebcabd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_audio_devices():\n",
    "    p = pyaudio.PyAudio()\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        print(device_info['name'])\n",
    "    p.terminate()\n",
    "\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fadae",
   "metadata": {},
   "source": [
    "### Main Code Cell\n",
    "#### Recording Audio using `pyAudio`\n",
    "#### Speech to Text using `Whisper`\n",
    "#### GPT Model: `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf184a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OPENAI_API():\n",
    "    \"\"\"\n",
    "    Loads the OpenAI API key from the environment variables.\n",
    "\n",
    "    Returns:\n",
    "    - str: The OpenAI API key.\n",
    "    \"\"\"\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai.api_key:\n",
    "        raise ValueError(\"OpenAI API key is not set. Please set the 'OPENAI_API_KEY' environment variable in your .env file.\")\n",
    "    return openai.api_key\n",
    "\n",
    "def get_Model(): \n",
    "    \"\"\"\n",
    "    Determines the appropriate GPT-3.5 model based on the current date.\n",
    "\n",
    "    Returns:\n",
    "    - str: The model name to use.\n",
    "    \n",
    "    Note: We are not using this function right now \n",
    "    because the process_prompt function already decides the model.\n",
    "    \"\"\"\n",
    "    current_date = datetime.datetime.now().date()\n",
    "    target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "    # Select the model based on the current date\n",
    "    if current_date > target_date:\n",
    "        llm_model = \"gpt-3.5-turbo\"\n",
    "    else:\n",
    "        llm_model = \"gpt-3.5-turbo-0301\"\n",
    "    return llm_model\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    Lists all available audio input devices.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of tuples containing device index, name, max input channels, and default sample rate.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    devices = []\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        devices.append((i, device_info['name'], device_info['maxInputChannels'], device_info['defaultSampleRate']))\n",
    "    p.terminate()\n",
    "    return devices\n",
    "\n",
    "def get_device_index_by_name(name): \n",
    "    \"\"\"\n",
    "    Finds the index of an audio device by its name.\n",
    "\n",
    "    Args:\n",
    "    - name (str): The name of the device.\n",
    "\n",
    "    Returns:\n",
    "    - int: The index of the device.\n",
    "    \n",
    "    Note: This is a helper function which will be used in getAudio().\n",
    "    \"\"\"\n",
    "    devices = list_audio_devices()\n",
    "    for index, device_name, _, _ in devices:\n",
    "        if name.lower() in device_name.lower():\n",
    "            return index\n",
    "    return None\n",
    "\n",
    "def getAudio(output_filename=\"recorded_speech.wav\", device_name=\"MacBook Pro Microphone\", chunk_size=1024, \n",
    "             format=pyaudio.paInt16, channels=1, rate=16000, silence_threshold=1000, silence_duration=5):\n",
    "    \"\"\"\n",
    "    Records audio until a period of silence is detected and saves it to a file.\n",
    "\n",
    "    Args:\n",
    "    - output_filename (str): Name of the output WAV file.\n",
    "    - device_name (str): Name of the input audio device.\n",
    "    - chunk_size (int): Number of frames per buffer.\n",
    "    - format: Audio format (e.g., pyaudio.paInt16).\n",
    "    - channels (int): Number of audio channels.\n",
    "    - rate (int): Sampling rate in Hz.\n",
    "    - silence_threshold (int): Amplitude threshold for silence detection.\n",
    "    - silence_duration (int): Duration of silence required to stop recording (in seconds).\n",
    "\n",
    "    Returns:\n",
    "    - str: The name of the saved audio file.\n",
    "    \n",
    "    Note: Start talking only when you see the message \"Please start speaking. Recording...\" \n",
    "    If your conversation/prompt is over, but Memoro continues to record, just interrupt it.\n",
    "    \"\"\"\n",
    "    device_index = get_device_index_by_name(device_name)\n",
    "    if device_index is None:\n",
    "        raise ValueError(f\"Device '{device_name}' not found.\")\n",
    "\n",
    "    # Variables to store audio frames and silence detection\n",
    "    audio_frames = []\n",
    "    silent_chunks = 0\n",
    "    max_silent_chunks = int(rate / chunk_size * silence_duration)\n",
    "\n",
    "    def is_silent(data, threshold=silence_threshold):\n",
    "        \"\"\"Returns 'True' if below the silence threshold.\"\"\"\n",
    "        max_amplitude = np.max(np.abs(data))\n",
    "        return max_amplitude < threshold\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        nonlocal silent_chunks, audio_frames\n",
    "        audio_frames.append(in_data)\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "        if is_silent(audio_data):\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        if silent_chunks > max_silent_chunks:\n",
    "            return (None, pyaudio.paComplete)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open stream\n",
    "        stream = p.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk_size,\n",
    "                        stream_callback=callback,\n",
    "                        input_device_index=device_index)\n",
    "\n",
    "        print(\"Please start speaking. Recording...\")\n",
    "        stream.start_stream()\n",
    "\n",
    "        # Keep the stream active while recording\n",
    "        while stream.is_active():\n",
    "            pass\n",
    "\n",
    "        # Stop and close the stream\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "    except KeyboardInterrupt: \n",
    "        # Handle keyboard interruption for noisy environments\n",
    "        print(\"Recording interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        p.terminate()\n",
    "\n",
    "    # Save the recorded audio to a file\n",
    "    try:\n",
    "        with wave.open(output_filename, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(audio_frames))\n",
    "        print(f\"Audio saved to {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save audio file: {e}\")\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def create_metadata(sentences, window=20, stride=4):\n",
    "    \"\"\"\n",
    "    Creates metadata for sentences with a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "    - sentences (list): List of tokenized sentences.\n",
    "    - window (int): Number of sentences in each metadata entry.\n",
    "    - stride (int): Step size between each metadata entry.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of metadata entries.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i in tqdm(range(0, len(sentences), stride)):\n",
    "        i_end = min(len(sentences), i + window)\n",
    "        text = ' '.join(sentences[i:i_end])\n",
    "        data.append({\n",
    "            'text': text,\n",
    "            'id': i,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def get_PINECONE_API():\n",
    "    \"\"\"\n",
    "    Initializes connection to Pinecone API and returns the index.\n",
    "\n",
    "    Returns:\n",
    "    - pinecone.Index: The Pinecone index.\n",
    "    \"\"\"\n",
    "    # Initialize connection to Pinecone\n",
    "    pinecone.api_key = os.getenv('PINECONE_API_KEY')\n",
    "    \n",
    "    index_name = 'memoro'\n",
    "    # Initialize connection to Pinecone\n",
    "    pc = Pinecone(api_key=pinecone.api_key)\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        # If the index does not exist, create it\n",
    "        pinecone.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine'\n",
    "        )\n",
    "    \n",
    "    # Connect to index\n",
    "    index = pc.Index(index_name)\n",
    "    return index\n",
    "\n",
    "def upsert_vectors(sentences, embed_model='text-embedding-ada-002'):\n",
    "    \"\"\"\n",
    "    Upserts vectors into Pinecone from sentences.\n",
    "\n",
    "    Args:\n",
    "    - sentences (list): List of tokenized sentences.\n",
    "    - embed_model (str): Model to use for generating embeddings.\n",
    "    \"\"\"\n",
    "    new_data = create_metadata(sentences)\n",
    "    batch_size = 100  # How many embeddings we create and insert at once\n",
    "\n",
    "    for i in tqdm(range(0, len(new_data), batch_size)):\n",
    "        # Find end of batch\n",
    "        i_end = min(len(new_data), i + batch_size)\n",
    "        meta_batch = new_data[i:i_end]\n",
    "        # Get IDs\n",
    "        ids_batch = [str(x['id']) for x in meta_batch]\n",
    "        # Get texts to encode\n",
    "        texts = [x['text'] for x in meta_batch]\n",
    "        # Create embeddings (try-except added to avoid RateLimitError)\n",
    "\n",
    "        try:\n",
    "            res = openai.Embedding.create(input=texts, model=embed_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embeddings: {e}\")\n",
    "            done = False\n",
    "            while not done:\n",
    "                sleep(5)\n",
    "                try:\n",
    "                    res = openai.Embedding.create(input=texts, model=embed_model)\n",
    "                    done = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Retrying due to error: {e}\")\n",
    "                    pass\n",
    "                \n",
    "        embeds = [record['embedding'] for record in res.to_dict()['data']]\n",
    "        # Cleanup metadata\n",
    "        meta_batch = [{'text': x['text']} for x in meta_batch]\n",
    "        \n",
    "        to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "        \n",
    "        index = get_PINECONE_API()\n",
    "        print('Connected to the Pinecone API')\n",
    "        # Upsert to Pinecone\n",
    "        try:\n",
    "            index.upsert(vectors=to_upsert)\n",
    "            print(f\"Upserted batch {i // batch_size + 1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error upserting batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "def speech_to_text():\n",
    "    \"\"\"\n",
    "    Converts recorded audio to text using Whisper model.\n",
    "\n",
    "    Returns:\n",
    "    - str: The transcribed text.\n",
    "    \"\"\"\n",
    "    audio = getAudio()\n",
    "\n",
    "    # Suppress the FP16 warning\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(\"base\")  \n",
    "    '''\n",
    "    Choose among tiny, base, small, medium, large models\n",
    "    The higher the model, higher the accuracy. But more accuracy means \n",
    "    it will take a lot longer to transcribe the audio.\n",
    "    '''\n",
    "\n",
    "    print('Processing speech...')\n",
    "    # Transcribe the audio file\n",
    "    result = model.transcribe(audio)\n",
    "    print('Transcribed!')\n",
    "    text = result['text']\n",
    "    write_to_file(text)\n",
    "    return text\n",
    "\n",
    "def process_context():\n",
    "    \"\"\"\n",
    "    Processes the context by recording speech, converting it to text, and upserting to Pinecone.\n",
    "    \"\"\"\n",
    "    text = speech_to_text()\n",
    "    print('Tokenizing...')\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    print('Upserting...')\n",
    "    upsert_vectors(sentences)\n",
    "    print('Upserted.')\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"\n",
    "    Converts text to speech and plays the audio.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be converted to speech.\n",
    "    \"\"\"\n",
    "    response = openai.Audio.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=text\n",
    "    )\n",
    "    response_path = os.path.join(os.getcwd(), 'response_voice.mp3')  # Contains the audio you hear when Memoro responds\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    response.stream_to_file(response_path)\n",
    "    play_audio(response_path)\n",
    "\n",
    "def write_to_file(text):\n",
    "    \"\"\"\n",
    "    Writes the text to a file.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The text to be written.\n",
    "\n",
    "    Returns:\n",
    "    - str: The file path.\n",
    "    \"\"\"\n",
    "    with open('STT_file.txt', 'w') as file:\n",
    "        file.write(text)\n",
    "        \n",
    "    return os.path.join(os.getcwd(), 'STT_file.txt')\n",
    "\n",
    "def read_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The read text.\n",
    "    \n",
    "    Note: We are not using this function right now and may discard it after \n",
    "    integrating Memoro with PineCone.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def play_audio(file_path):\n",
    "    \"\"\"\n",
    "    Plays an audio file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the audio file.\n",
    "    \"\"\"\n",
    "    playsound(file_path)\n",
    "    \n",
    "def get_prompt(embed_model='text-embedding-ada-002'):\n",
    "    \"\"\"\n",
    "    Retrieves the prompt by converting speech to text and finding relevant contexts from Pinecone.\n",
    "\n",
    "    Args:\n",
    "    - embed_model (str): The embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "    - str: The constructed prompt.\n",
    "    \"\"\"\n",
    "    limit = 3750\n",
    "    intro_path = os.path.join(os.getcwd(), 'intro_prompt_voice.mp3')\n",
    "    play_audio(intro_path)\n",
    "    query = speech_to_text()\n",
    "    print(\"Recognized Prompt:\", query)\n",
    "    \n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        model=embed_model\n",
    "    )\n",
    "    \n",
    "    index = get_PINECONE_API()\n",
    "    # Retrieve from Pinecone\n",
    "    xq = res.to_dict()['data'][0]['embedding']\n",
    "    \n",
    "    # Get relevant contexts\n",
    "    res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # Build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\" +\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # Append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts) - 1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "def process_prompt():\n",
    "    \"\"\"\n",
    "    Processes the prompt by generating a response from the GPT-3.5-turbo model and converting it to speech.\n",
    "    \"\"\"\n",
    "    prompt = get_prompt()\n",
    "    # Query gpt-3.5-turbo\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a memory assistant listening to my conversations.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    text = response['choices'][0]['message']['content']\n",
    "    text_to_speech(text)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "api = get_OPENAI_API()\n",
    "context = process_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84008d4",
   "metadata": {},
   "source": [
    "### Run the cell below to prompt Memoro II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26334429",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_prompt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
